{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The llm-dataset-converter library (and its dependent libraries) can be used for converting Large Language Model (LLM) datasets from one format into another. It has support for the following domains: Pretrain Supervised Classification Pairs (Q&A, P/R) Translation Please refer to the dataset formats section for more details on supported formats. But the library does not just convert datasets, you can also slot in complex filter pipelines to process/clean the data. On this website you can find examples for: Downloader usage General usage Processing multiple files Locating files Compression Filter usage Docker usage Examples for the additional libraries: Faster whisper Google HTML MS Word (doc) MS Word (docx) OpenAI PDF TinT","title":"Home"},{"location":"compression/","text":"The output gets automatically compressed (when the format supports that), based on the extension that you use for the output. The following uses Gzip to compress the CSV file generated from the Alpaca JSON dataset: llm-convert \\ from-alpaca \\ --input ./alpaca_data_cleaned.json \\ to-csv-pr \\ --output alpaca_data_cleaned.csv.gz The input gets automatically decompressed based on its extension, provided the format supports that.","title":"Compression"},{"location":"doc/","text":"Requirements # The ldc-doc library. The antiword binary available on PATH Debian/Ubuntu: sudo apt install antiword Windows: Softpedia Plugins # Extracting text from MS Word (.doc) documents # ldc-convert \\ -l INFO \\ from-doc-pt \\ -l INFO \\ --input \"./input/*.doc\" \\ to-txt-pt \\ -l INFO \\ --output \"./output/\"","title":"MS Word (doc)"},{"location":"doc/#requirements","text":"The ldc-doc library. The antiword binary available on PATH Debian/Ubuntu: sudo apt install antiword Windows: Softpedia","title":"Requirements"},{"location":"doc/#plugins","text":"","title":"Plugins"},{"location":"doc/#extracting-text-from-ms-word-doc-documents","text":"ldc-convert \\ -l INFO \\ from-doc-pt \\ -l INFO \\ --input \"./input/*.doc\" \\ to-txt-pt \\ -l INFO \\ --output \"./output/\"","title":"Extracting text from MS Word (.doc) documents"},{"location":"docker/","text":"Below are examples for using the llm-dataset-converter library via its Docker images . Interactive session # The following command starts an interactive session, mapping the current working directory to /workspace : docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/llm-dataset-converter:latest Conversion pipeline # The following converts the Alpaca dataset from JSON into CSV format: docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/llm-dataset-converter:latest \\ llm-convert \\ -l INFO \\ from-alpaca \\ --input /workspace/alpaca_data_cleaned.json \\ -l INFO \\ to-csv-pr \\ --output /workspace/alpaca_data_cleaned.csv -l INFO NB: The input and output directories are located below the current working directory ( pwd ).","title":"Docker usage"},{"location":"docker/#interactive-session","text":"The following command starts an interactive session, mapping the current working directory to /workspace : docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/llm-dataset-converter:latest","title":"Interactive session"},{"location":"docker/#conversion-pipeline","text":"The following converts the Alpaca dataset from JSON into CSV format: docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/llm-dataset-converter:latest \\ llm-convert \\ -l INFO \\ from-alpaca \\ --input /workspace/alpaca_data_cleaned.json \\ -l INFO \\ to-csv-pr \\ --output /workspace/alpaca_data_cleaned.csv -l INFO NB: The input and output directories are located below the current working directory ( pwd ).","title":"Conversion pipeline"},{"location":"docx/","text":"Requirements # Requires the ldc-docx library. Plugins # Extracting text from MS Word (.docx) documents # ldc-convert \\ -l INFO \\ from-docx-pt \\ -l INFO \\ --input \"./input/*.docx\" \\ to-txt-pt \\ -l INFO \\ --output \"./output/\"","title":"MS Word (docx)"},{"location":"docx/#requirements","text":"Requires the ldc-docx library.","title":"Requirements"},{"location":"docx/#plugins","text":"","title":"Plugins"},{"location":"docx/#extracting-text-from-ms-word-docx-documents","text":"ldc-convert \\ -l INFO \\ from-docx-pt \\ -l INFO \\ --input \"./input/*.docx\" \\ to-txt-pt \\ -l INFO \\ --output \"./output/\"","title":"Extracting text from MS Word (.docx) documents"},{"location":"downloaders/","text":"The following command downloads the file vocab.json from the Hugging Face project lysandre/arxiv-nlp : llm-download \\ huggingface \\ -l INFO \\ -i lysandre/arxiv-nlp \\ -f vocab.json \\ -o . The next command gets the file part_1_200000.parquet from the dataset nampdn-ai/tiny-codes (if you don't specify a filename, the complete dataset will get downloaded): llm-download \\ huggingface \\ -l INFO \\ -i nampdn-ai/tiny-codes \\ -t dataset \\ -f part_1_200000.parquet \\ -o . NB: Hugging Face will cache files locally in your home directory before copying it to the location that you specified.","title":"Downloader usage"},{"location":"faster_whisper/","text":"Requirements # Requires the ldc-faster-whisper library. Plugins # Transcribing audio # The following commands transcribes raw audio files using faster-whisper ( from-fwaudio-pt ), with the result then being stored in pretrain text format: ldc-convert \\ -l INFO \\ from-fwaudio-pt -l INFO \\ --input \"./input/*.wav\" \\ -1 \\ to-txt-pt \\ -l INFO \\ --output \"./output/\"","title":"Faster whisper"},{"location":"faster_whisper/#requirements","text":"Requires the ldc-faster-whisper library.","title":"Requirements"},{"location":"faster_whisper/#plugins","text":"","title":"Plugins"},{"location":"faster_whisper/#transcribing-audio","text":"The following commands transcribes raw audio files using faster-whisper ( from-fwaudio-pt ), with the result then being stored in pretrain text format: ldc-convert \\ -l INFO \\ from-fwaudio-pt -l INFO \\ --input \"./input/*.wav\" \\ -1 \\ to-txt-pt \\ -l INFO \\ --output \"./output/\"","title":"Transcribing audio"},{"location":"filters/","text":"Instead of just reading and writing the data records, you can also inject filters in between them. E.g., the following command-line will load the Alpaca JSON dataset and only keep records that have the keyword function in either the instruction , input or output data of the record: llm-convert \\ -l INFO \\ from-alpaca \\ -l INFO \\ --input alpaca_data_cleaned.json \\ keyword \\ -l INFO \\ --keyword function \\ --location any \\ --action keep \\ to-alpaca \\ -l INFO \\ --output alpaca_data_cleaned-filtered.json NB: When chaining filters, the tool checks whether accepted input and generated output is compatible (including from reader/writer).","title":"Filter usage"},{"location":"general_usage/","text":"A conversion pipeline is executed via the llm-convert tool, with a pipeline typically consisting of a reader and writer, with optional filters in-between. However, only the reader is required. The following loads the Alpaca JSON file and stores it in CSV format (P/R): llm-convert \\ from-alpaca \\ --input ./alpaca_data_cleaned.json \\ to-csv-pr \\ --output alpaca_data_cleaned.csv If you want some logging output, e.g., on progress and what files are being processed/generated: llm-convert \\ -l INFO \\ from-alpaca \\ --input ./alpaca_data_cleaned.json \\ -l INFO \\ to-csv-pr \\ --output alpaca_data_cleaned.csv -l INFO","title":"General usage"},{"location":"google/","text":"Requirements # Requires the ldc-google library. Plugins # Translating text using the Google Translate API # The following translates the Alpaca JSON dataset from English into M\u0101ori: ldc-convert \\ -l INFO \\ -u 100 \\ from-alpaca \\ -l INFO \\ --input \"./input/alpaca_data_cleaned.json\" \\ google-translate \\ -l INFO \\ -p PROJECTNAME \\ -s en \\ -t mi \\ to-alpaca \\ -l INFO \\ --output \"./output/alpaca_data_cleaned-mi.json\" \\ --pretty_print \\ --ensure_ascii NB: Replace PROJECTNAME with your own Google project ID. Requires local dev credentials for Google's cloud API: https://cloud.google.com/docs/authentication/provide-credentials-adc#local-dev","title":"Google"},{"location":"google/#requirements","text":"Requires the ldc-google library.","title":"Requirements"},{"location":"google/#plugins","text":"","title":"Plugins"},{"location":"google/#translating-text-using-the-google-translate-api","text":"The following translates the Alpaca JSON dataset from English into M\u0101ori: ldc-convert \\ -l INFO \\ -u 100 \\ from-alpaca \\ -l INFO \\ --input \"./input/alpaca_data_cleaned.json\" \\ google-translate \\ -l INFO \\ -p PROJECTNAME \\ -s en \\ -t mi \\ to-alpaca \\ -l INFO \\ --output \"./output/alpaca_data_cleaned-mi.json\" \\ --pretty_print \\ --ensure_ascii NB: Replace PROJECTNAME with your own Google project ID. Requires local dev credentials for Google's cloud API: https://cloud.google.com/docs/authentication/provide-credentials-adc#local-dev","title":"Translating text using the Google Translate API"},{"location":"html/","text":"Requirements # Requires the ldc-html library. Plugins # Extracting text from HTML files # ldc-convert \\ -l INFO \\ from-html-pt \\ -l INFO \\ --input \"./input/*.html\" \\ -s \"\\n\" \\ to-txt-pt \\ -l INFO \\ --output \"./output/\"","title":"HTML"},{"location":"html/#requirements","text":"Requires the ldc-html library.","title":"Requirements"},{"location":"html/#plugins","text":"","title":"Plugins"},{"location":"html/#extracting-text-from-html-files","text":"ldc-convert \\ -l INFO \\ from-html-pt \\ -l INFO \\ --input \"./input/*.html\" \\ -s \"\\n\" \\ to-txt-pt \\ -l INFO \\ --output \"./output/\"","title":"Extracting text from HTML files"},{"location":"locating_files/","text":"The following command scans the /some/dir directory recursively for .txt files that do not have raw in the file path: llm-find \\ -l INFO \\ -i /some/dir/ -r \\ -m \".*\\.txt\" \\ -n \".*\\/raw\\/.*\" \\ -o ./files.txt The same command, but splitting the files into training, validation and test lists, using a ratio of 70/15/15: llm-find \\ -l INFO \\ -i /some/dir/ -r \\ -m \".*\\.txt\" \\ -n \".*\\/raw\\/.*\" \\ --split_ratios 70 15 15 \\ --split_names train val test \\ -o ./files.txt This results in the following three files: files-train.txt , files-val.txt and files-test.txt .","title":"Locating files"},{"location":"openai/","text":"Requirements # Requires the ldc-openai library. Plugins # Count tokens # In order to determine how costly the OpenAI usage is, you can apply the openai-count-tokens filter to your data (in the example below, 1k tokens cost $0.002): ldc-convert \\ -l INFO \\ from-alpaca \\ -l INFO \\ --input \"./input/alpaca_data_cleaned.json\" \\ openai-count-tokens \\ -l INFO \\ -m gpt-3.5-turbo \\ -t 0.002 \\ -p \"You will be provided with a sentence in English, and your task is to translate it into M\u0101ori.\" Will output something like this: INFO:openai-count-tokens:# tokens (prompt): 22 INFO:openai-count-tokens:# tokens: 11512740 INFO:openai-count-tokens:total price (1k tokens = 0.002000): 23.025480","title":"OpenAI"},{"location":"openai/#requirements","text":"Requires the ldc-openai library.","title":"Requirements"},{"location":"openai/#plugins","text":"","title":"Plugins"},{"location":"openai/#count-tokens","text":"In order to determine how costly the OpenAI usage is, you can apply the openai-count-tokens filter to your data (in the example below, 1k tokens cost $0.002): ldc-convert \\ -l INFO \\ from-alpaca \\ -l INFO \\ --input \"./input/alpaca_data_cleaned.json\" \\ openai-count-tokens \\ -l INFO \\ -m gpt-3.5-turbo \\ -t 0.002 \\ -p \"You will be provided with a sentence in English, and your task is to translate it into M\u0101ori.\" Will output something like this: INFO:openai-count-tokens:# tokens (prompt): 22 INFO:openai-count-tokens:# tokens: 11512740 INFO:openai-count-tokens:total price (1k tokens = 0.002000): 23.025480","title":"Count tokens"},{"location":"pdf/","text":"Requirements # Requires the ldc-pdf library. Plugins # Extracting text from PDFs # The following pipeline extracts the text from a thesis PDF, skipping the first and last page ( -p ): ldc-convert \\ -l INFO \\ from-pdf-pt \\ -l INFO \\ --input \"./input/thesis.pdf\" \\ -p 2-last_1 \\ to-txt-pt \\ -l INFO \\ --output \"./output/thesis.txt\"","title":"PDF"},{"location":"pdf/#requirements","text":"Requires the ldc-pdf library.","title":"Requirements"},{"location":"pdf/#plugins","text":"","title":"Plugins"},{"location":"pdf/#extracting-text-from-pdfs","text":"The following pipeline extracts the text from a thesis PDF, skipping the first and last page ( -p ): ldc-convert \\ -l INFO \\ from-pdf-pt \\ -l INFO \\ --input \"./input/thesis.pdf\" \\ -p 2-last_1 \\ to-txt-pt \\ -l INFO \\ --output \"./output/thesis.txt\"","title":"Extracting text from PDFs"},{"location":"processing_multiple_files/","text":"Provided that the reader supports, you can also process multiple files, one after the other. For that you either specify them explicitly (multiple arguments to the --input option) or use a glob syntax (e.g., --input \"*.json\" ). For the latter, you should surround the argument with double quotes to avoid the shell expanding the names automatically. If you have a lot of files, it will be more efficient to store these in text files (with one file per line) and pass these to the reader using the --input_list option (assuming that the reader supports this). Such file lists can be generated with the llm-find tool. See Locating files for examples. As for specifying the output, you simply specify the output directory. An output file name gets automatically generated from the name of the current input file that is being processed. If you want to compress the output files, you need to specify your preferred compression format via the global -c/--compression option of the llm-convert tool. By default, no compression is used. Please note, that when using a stream writer (e.g., for text or jsonlines output) in conjunction with an output directory, each record will be stored in a separate file. In order to transfer all the records into a single file, you have to explicitly specify that file as output.","title":"Processing multiple files"},{"location":"tint/","text":"Requirements # Requires the ldc-tint library. Plugins # Detect M\u0101ori language # The detect-maori can be used for filtering out records that don't have enough M\u0101ori characters or too many non-M\u0101ori ones by applying user-supplied threshold. Below, a maximum on 10% of non-M\u0101ori characters are tolerated before the record gets discarded: ldc-convert \\ -l INFO \\ from-alpaca \\ -l INFO \\ -i \"./input/alpaca_data_cleaned-mi.json\" \\ detect-maori \\ -l INFO \\ -M 0.1 An alternative filter is-maori uses the reo-toolkit to determine whether text is M\u0101ori or not. Below, a minimum of 70% words must be M\u0101ori: ldc-convert \\ -l INFO \\ from-alpaca \\ -l INFO \\ -i \"./input/alpaca_data_cleaned-mi.json\" is-maori \\ -l INFO \\ -m 0.7 Handling macrons # You can use the de-macronize filter to remove/replace macrons: ldc-convert \\ -l INFO \\ from-txt-pt \\ -l INFO \\ -i \"./input/m\u0101ori.txt\" \\ de-macronize \\ -d strip \\ -l INFO \\ to-txt-pt \\ -l INFO \\ -o \"./output/maori_stripped.txt\"","title":"TinT"},{"location":"tint/#requirements","text":"Requires the ldc-tint library.","title":"Requirements"},{"location":"tint/#plugins","text":"","title":"Plugins"},{"location":"tint/#detect-maori-language","text":"The detect-maori can be used for filtering out records that don't have enough M\u0101ori characters or too many non-M\u0101ori ones by applying user-supplied threshold. Below, a maximum on 10% of non-M\u0101ori characters are tolerated before the record gets discarded: ldc-convert \\ -l INFO \\ from-alpaca \\ -l INFO \\ -i \"./input/alpaca_data_cleaned-mi.json\" \\ detect-maori \\ -l INFO \\ -M 0.1 An alternative filter is-maori uses the reo-toolkit to determine whether text is M\u0101ori or not. Below, a minimum of 70% words must be M\u0101ori: ldc-convert \\ -l INFO \\ from-alpaca \\ -l INFO \\ -i \"./input/alpaca_data_cleaned-mi.json\" is-maori \\ -l INFO \\ -m 0.7","title":"Detect M\u0101ori language"},{"location":"tint/#handling-macrons","text":"You can use the de-macronize filter to remove/replace macrons: ldc-convert \\ -l INFO \\ from-txt-pt \\ -l INFO \\ -i \"./input/m\u0101ori.txt\" \\ de-macronize \\ -d strip \\ -l INFO \\ to-txt-pt \\ -l INFO \\ -o \"./output/maori_stripped.txt\"","title":"Handling macrons"}]}